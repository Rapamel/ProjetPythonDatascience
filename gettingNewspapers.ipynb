{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook pour importer les données \n",
    "\n",
    "Source : https://huggingface.co/datasets/dell-research-harvard/AmericanStories/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "url=\"https://huggingface.co/api/datasets/dell-research-harvard/AmericanStories\"\n",
    "token=os.getenv(\"HuggingFaceToken\")\n",
    "headers = {\n",
    "    \"Authorization\" : f\"Bearer {token}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_request(fileName):\n",
    "    url= f\"https://huggingface.co/datasets/dell-research-harvard/AmericanStories/resolve/main/{fileName}\"\n",
    "    print(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/dell-research-harvard/AmericanStories/resolve/main/faro_1944.tar.gz\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "wb=requests.get(url_request(\"faro_1944.tar.gz\"), headers=headers)\n",
    "print(wb.status_code)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"faro_1944.tar.gz\", \"wb\") as f:\n",
    "    f.write(wb.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"faro_1944.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(path=\"faro_1944\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcous de tout les JSON du dossier pour récupérer tout les noms de journaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33138\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "year=1929\n",
    "files=os.listdir(f\"ArticlesTarGz/faro_{year}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{year}\")\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lccn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull articles\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 11\u001b[0m     newspapers\u001b[38;5;241m.\u001b[39mappend((\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlccn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m],j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lccn'"
     ]
    }
   ],
   "source": [
    "\n",
    "n=0\n",
    "newspapers=[]\n",
    "for i in files:\n",
    "    n+=1\n",
    "    if n%1000==0:\n",
    "        print(n, end=\" \")\n",
    "    #open the json file and get the newspaper name\n",
    "    with open(f\"ArticlesTarGz/faro_{year}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{year}/{i}\") as f:\n",
    "        data = json.load(f)\n",
    "    for j in data['full articles']:\n",
    "        newspapers.append((data[\"lccn\"][\"title\"],j['article']))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lccn', 'edition', 'page_number', 'scan', 'bboxes', 'full articles'])\n",
      "Evening star.\n",
      "1929-01-01\n"
     ]
    }
   ],
   "source": [
    "file=files[0]\n",
    "with open(f\"ArticlesTarGz/faro_{year}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{year}/{file}\") as f:\n",
    "    data = json.load(f)\n",
    "print(data.keys())\n",
    "print(data['lccn']['title'])\n",
    "print(data[\"edition\"][\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433769\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "print(len(newspapers))\n",
    "df=pd.DataFrame(newspapers, columns=[\"newspaper\",\"article\"])\n",
    "df=df[df[\"article\"].str.contains(\"inflation\",case=False)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     newspaper  \\\n",
      "30    The Daily Alaska empire.   \n",
      "103             The messenger.   \n",
      "1193             Evening star.   \n",
      "1565   The Waterbury Democrat.   \n",
      "3259    Imperial Valley press.   \n",
      "\n",
      "                                                article  \n",
      "30    cific-\"With Gods help, we are on Jour way back...  \n",
      "103   \"The one outstanding news development of 1943 ...  \n",
      "1193  MEXICO CITY, Jan. 3 LP.-An\\nanti-inflation pro...  \n",
      "1565  OPA continues it's indefensible fiat that an e...  \n",
      "3259  and still maintain the American\\nstandards\\n\\n...  \n",
      "cific-\"With Gods help, we are on Jour way back\" declared General Macarthur after the Allied victory at Lac Sept 16) That victory was only one in long series from the capture of Guadalcanal tFeb. lW to the reconquest of the Gilberts in ovember. They broke Japan's grip in the south Pacific and opened the way for a grand offen- sive. Admiral Nimita called the Gil- bert invasion another road to To- kyo.' and added, in due time we'll have enough equipment to travel all the roads\"\n",
      "\n",
      " 6. Mussolini Topples tJuly 25)-- It was one down and two to go when the Balcony Caesar was tum- bled from his high estate in one of the most startling and unexpect- ed single news events of the year Latest photographs of Mussolini, following his rescue show a shamefaced and apathetic old man travelling down Oblivion Street.\n",
      "\n",
      " T. Pay-As- You-Go Taxes Voted\n",
      "\n",
      " tJune 2)--Anything that hits the American pocketbook resounding whack is rfews. Starting July 1, Uncle Sam put his hand into vir- tually every wage-earner's pay en- velope for twenty percent of the kitty This was one of many strin- gent home front moves in sec saw hold the line\" battle against inflation\n",
      "\n",
      " 8. Singular Murder Case Crowds War News-Once in generation a spectacular crime riyets global at tention. War times are no exception. When fabulously wealthy Sir Harry Cakes was found slain in Nassau, the world got-and relished-an old fashioned crime sensation crowded with mystery, money, romance, sex and the inevitable finger print. The acquittal of son-in-law Alfred de Marigny of murder charges (Nov lD left the mystery as deep as ever.\n",
      "\n",
      " 1 g. Coal Strikes Imperil War Pro- duction--A seven- months wage dis pute which shut down the nations i soft coal mines four times and sent Uncle Sam into the pits twice, end ed in a formula giving an increase in earnings\" of $l.50, day to John L. Lewis miners. In addition, this most serious internal conflict in a nation at war cost more than 4(\n",
      "\n",
      " million tons of coal, caused enact ment of anti-strike legislation, and boosted coal prices.\n",
      "\n",
      " 10. U-Boats Lose Battle of At { lantic-The early months of the\n",
      "\n",
      " war brought tragic toll of sub- marine sinkings. Gradually condi-\n",
      "\n",
      " tions bettered until it could be an nounced that U-boats were being\n",
      "\n",
      " knocked off at the rate of one a\n",
      "\n",
      " day and that no sinkings had oc- cured in the North Atlantic in a\n",
      "\n",
      " three month period. Beating the \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df[\"article\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions pour manipuler les données \n",
    "\n",
    "Maintenant nous allons définir quelques fonctions pour intéragir simplement avec la base de données sans avoir à tout télécharger car le volume des données rend le téléchargement intégral difficile.\n",
    "\n",
    "Ces fonctions définies ci-dessous seront aussi définies dans le fichier *getData.py* pour pouvoir être importées dans le notebook principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_credentials renvoie le header contenant le token pour pouvoir requêter l'API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials():\n",
    "    load_dotenv()\n",
    "    url=\"https://huggingface.co/api/datasets/dell-research-harvard/AmericanStories\"\n",
    "    token=os.getenv(\"HuggingFaceToken\")\n",
    "    headers = {\n",
    "        \"Authorization\" : f\"Bearer {token}\"\n",
    "    }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download_targz prend en argument les années que l'on veut télécharger et les télécharge dans le dossier *ArticlesTarGz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_targz(years):\n",
    "    if isinstance(years, str):\n",
    "        years=[years]\n",
    "    os.makedirs(\"ArticlesTarGz\", exist_ok=True)\n",
    "    for i in years:\n",
    "        print(i, end=\" \")\n",
    "        file=f\"faro_{i}.tar.gz\"\n",
    "        url=f\"https://huggingface.co/datasets/dell-research-harvard/AmericanStories/resolve/main/{file}\"\n",
    "        wb=requests.get(url, headers=headers)\n",
    "        print(wb.status_code)\n",
    "        if wb.ok:\n",
    "            with open(\"ArticlesTarGz/\"+file, \"wb\") as f:\n",
    "                f.write(wb.content)\n",
    "        else:\n",
    "            print(f\"Error {wb.status_code} downloading {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929 200\n",
      "1930 200\n"
     ]
    }
   ],
   "source": [
    "headers=load_credentials()\n",
    "download_targz([i for i in range(1929,1931)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targz(years):\n",
    "    if isinstance(years, str):\n",
    "        years=[years]\n",
    "    for i in years:\n",
    "        print(i, end=\" \")\n",
    "        try :\n",
    "            with tarfile.open(f\"ArticlesTarGz/faro_{i}.tar.gz\", \"r:gz\") as tar:\n",
    "                tar.extractall(path=f\"ArticlesTarGz/faro_{i}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error extracting {i} The file is not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929 1930 "
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "extract_targz([i for i in range(1929,1931)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_left_local(fonction, acc, years):\n",
    "    if isinstance(years, str):\n",
    "        years=[years]\n",
    "    for i in years:\n",
    "        print(i, end=\" \")\n",
    "        files=os.listdir(f\"ArticlesTarGz/faro_{i}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{i}\")\n",
    "        for j in files:\n",
    "            with open(f\"ArticlesTarGz/faro_{i}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{i}/{j}\") as f:\n",
    "                data = json.load(f)\n",
    "            acc=fonction(data, acc, (data[\"lccn\"][\"title\"],data['edition']['date'],data['full articles']))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_local(fonction,years):\n",
    "    if isinstance(years, str):\n",
    "        years=[years]\n",
    "    acc=[]\n",
    "    for i in years:\n",
    "        print(i, end=\" \")\n",
    "        files=os.listdir(f\"ArticlesTarGz/faro_{i}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{i}\")\n",
    "        for j in files:\n",
    "            with open(f\"ArticlesTarGz/faro_{i}/mnt\\\\122a7683-fa4b-45dd-9f13-b18cc4f4a187/ca_rule_based_fa_clean/faro_{i}/{j}\") as f:\n",
    "                data = json.load(f)\n",
    "            if data['full articles']!=[]:\n",
    "                try :\n",
    "                    acc.append(fonction(data[\"lccn\"][\"title\"],data['edition']['date'][:-3],data['full articles']))\n",
    "                except:\n",
    "                    acc.append(fonction(\"No titles found\",data['edition']['date'][:-3],data['full articles']))\n",
    "    return acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_freq(df,isInflation,title,date,articles):\n",
    "    if not (date in df.keys()):\n",
    "        df[date]=0,0\n",
    "    for i in range(len(articles)): \n",
    "        f,n=df[date]\n",
    "        if isInflation(articles[i]['article']):            \n",
    "            df[date]=(f+1,n+1)\n",
    "        else :\n",
    "            df[date]=(f,n+1)\n",
    "            articles[i]=None\n",
    "    articles=[i for i in articles if i is not None]\n",
    "    return (title,date,articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "frequences={}\n",
    "def isInflation(article):\n",
    "    return article.lower().find(\"inflation\")!=-1\n",
    "f_f=partial(filter_and_freq, frequences, isInflation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "f_f(\"newspaper\",\"date\"[:-3],[{\"article\" : \"inflation is a problem in the US\"},{\"article\" : \"bread is a problem in the US\"}])\n",
    "print(frequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929 1930 "
     ]
    }
   ],
   "source": [
    "inflationArticles=map_local(f_f,[i for i in range(1929,1931)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences=pd.DataFrame(frequences).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0      1\n",
      "d        1      2\n",
      "1929-01  1  34921\n",
      "1929-02  0  32626\n",
      "1929-03  0  38402\n",
      "1929-04  0  45940\n",
      "1929-05  0  42844\n",
      "1929-06  0  46991\n",
      "1929-07  0  44949\n",
      "1929-08  0  45022\n",
      "1929-09  0  35326\n",
      "1929-10  0  40179\n",
      "1929-11  0  46365\n",
      "1929-12  0  46530\n",
      "1930-01  0  37954\n",
      "1930-02  0  35029\n",
      "1930-03  0  41033\n",
      "1930-04  1  38407\n",
      "1930-05  0  39827\n",
      "1930-06  0  44111\n",
      "1930-07  0  40374\n",
      "1930-08  0  40223\n",
      "1930-09  0  40325\n",
      "1930-10  0  44699\n",
      "1930-11  1  39701\n",
      "1930-12  2  51012\n"
     ]
    }
   ],
   "source": [
    "print(frequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
